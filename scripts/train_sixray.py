#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ SIXray —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU
–ü—É—Ç—å: scripts/train_sixray.py
"""

import os
import sys
from pathlib import Path
import torch
import torch.cuda as cuda

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from ultralytics import YOLO
import yaml
import argparse

class SIXRayTrainer:
    def __init__(self, data_yaml_path, model_type='yolov8m.pt'):
        self.data_yaml_path = Path(data_yaml_path)
        self.model_type = model_type
        self.model = None
        self.project_root = Path(__file__).parent.parent
        
    def setup_environment(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ GPU"""
        print("üîç –ü–†–û–í–ï–†–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø –ò –ù–ê–°–¢–†–û–ô–ö–ê GPU...")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
        print(f"üêç PyTorch version: {torch.__version__}")
        print(f"‚ö° CUDA available: {cuda.is_available()}")
        
        device = 'gpu'  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        if cuda.is_available():
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
            gpu_count = cuda.device_count()
            print(f"üéÆ –î–æ—Å—Ç—É–ø–Ω–æ GPU: {gpu_count}")
            
            for i in range(gpu_count):
                gpu_name = cuda.get_device_name(i)
                gpu_memory = cuda.get_device_properties(i).total_memory / 1e9
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º GPU
            device = 0  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é GPU
            torch.cuda.set_device(device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GPU –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            current_device = cuda.current_device()
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {current_device} - {cuda.get_device_name(current_device)}")
            
            # –¢–µ—Å—Ç GPU
            test_tensor = torch.tensor([1.0, 2.0, 3.0]).cuda()
            print(f"‚úÖ GPU —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: —Ç–µ–Ω–∑–æ—Ä –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {test_tensor.device}")
            
        else:
            print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
            print("   - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ CUDA Toolkit")
            print("   - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ torch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA")
            print("   - –î–æ—Å—Ç—É–ø–Ω—ã –ª–∏ –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA")
            print("   - –ó–∞–ø—É—â–µ–Ω –ª–∏ Docker —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è data.yaml
        if not self.data_yaml_path.exists():
            raise FileNotFoundError(f"‚ùå –§–∞–π–ª {self.data_yaml_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
        with open(self.data_yaml_path, 'r') as f:
            data_config = yaml.safe_load(f)
            
        base_path = Path(data_config['path'])
        print(f"üìÅ –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞: {base_path}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–ø–æ–∫
        required_dirs = [
            base_path / 'train' / 'images',
            base_path / 'train' / 'labels',
            base_path / 'valid' / 'images', 
            base_path / 'valid' / 'labels',
            base_path / 'test' / 'images'
        ]
        
        for dir_path in required_dirs:
            if not dir_path.exists():
                print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {dir_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            else:
                file_count = len(list(dir_path.glob('*')))
                print(f"‚úÖ {dir_path}: {file_count} —Ñ–∞–π–ª–æ–≤")
                
        return device
    
    def setup_model(self, device):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–∞ GPU"""
        print("\nüîÑ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ù–ê GPU...")
        
        # –ü—É—Ç—å –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        model_path = self.project_root / 'models' / self.model_type
        
        if model_path.exists():
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {model_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            if device != 'cpu':
                self.model = YOLO(str(model_path)).to('cuda')
            else:
                self.model = YOLO(str(model_path))
        else:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_type}...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            if device != 'cpu':
                self.model = YOLO(self.model_type).to('cuda')
            else:
                self.model = YOLO(self.model_type)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–æ–¥–µ–ª—å
        print(f"üìç –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞: {next(self.model.model.parameters()).device}")
            
        return self.model
    
    def force_gpu_usage(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU"""
        if cuda.is_available():
            # –û—á–∏—â–∞–µ–º –∫—ç—à CUDA
            cuda.empty_cache()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            
            print("‚úÖ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
        else:
            print("‚ö†Ô∏è  CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è CPU –≤—ã—á–∏—Å–ª–µ–Ω–∏—è")
    
    def train(self, epochs=100, imgsz=640, batch=16):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU"""
        print("\nüéØ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ù–ê SIXray –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú GPU...")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
        self.force_gpu_usage()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        device = self.setup_environment()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        model = self.setup_model(device)
        
        # –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        runs_dir = self.project_root / 'runs'
        runs_dir.mkdir(exist_ok=True)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {runs_dir}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å GPU
        train_kwargs = {
            'data': str(self.data_yaml_path),
            'epochs': epochs,
            'imgsz': imgsz,
            'batch': batch,
            'workers': 8,
            'optimizer': 'AdamW',
            'lr0': 0.001,
            'patience': 15,
            'save': True,
            'exist_ok': True,
            'pretrained': True,
            'project': str(runs_dir),
            'name': 'detect/train_sixray_gpu',
            # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤
            'hsv_h': 0.015,
            'hsv_s': 0.7, 
            'hsv_v': 0.4,
            'translate': 0.1,
            'scale': 0.5,
            'fliplr': 0.5,
            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            'weight_decay': 0.0005,
            'warmup_epochs': 3.0,
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
            'box': 7.5,
            'cls': 0.5,
            'dfl': 1.5
        }
        
        # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        if device != 'cpu':
            train_kwargs['device'] = device
            print(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU: {device}")
        else:
            train_kwargs['device'] = 'cpu'
            print("‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –Ω–∞ CPU (GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("üî• –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø...")
        results = model.train(**train_kwargs)
        
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        
        # –ü—É—Ç—å –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        best_model_path = runs_dir / 'detect' / 'train_sixray_gpu' / 'weights' / 'best.pt'
        print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}")
        
        return results, best_model_path

def check_cuda_installation():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ CUDA –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    print("üîç –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê CUDA...")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–π
    print(f"PyTorch CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {cuda.is_available()}")
    if cuda.is_available():
        print(f"PyTorch CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {cuda.device_count()}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥–æ–π GPU
        for i in range(cuda.device_count()):
            print(f"GPU {i}: {cuda.get_device_name(i)}")
            print(f"  –ü–∞–º—è—Ç—å: {cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")
            print(f"  –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {cuda.get_device_properties(i).major}.{cuda.get_device_properties(i).minor}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ nvidia-smi (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ nvidia-smi –¥–æ—Å—Ç—É–ø–µ–Ω")
            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            lines = result.stdout.split('\n')
            if len(lines) > 0:
                print(f"  nvidia-smi: {lines[0]}")
        else:
            print("‚ùå nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    except:
        print("‚ùå nvidia-smi –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

def main():
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ YOLO –Ω–∞ SIXray —Å GPU')
    parser.add_argument('--epochs', type=int, default=100, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--batch', type=int, default=16, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--imgsz', type=int, default=640, help='–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è')
    parser.add_argument('--model', type=str, default='yolov8m.pt', help='–ú–æ–¥–µ–ª—å YOLO')
    parser.add_argument('--check-cuda', action='store_true', help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É CUDA')
    
    args = parser.parse_args()
    
    if args.check_cuda:
        check_cuda_installation()
        return
    
    # –ê–ë–°–û–õ–Æ–¢–ù–´–ô –ø—É—Ç—å –∫ data.yaml
    data_yaml_path = "/Users/bordvizov/Desktop/Python/xraydetect/data/SIXray/data.yaml"
    
    print("üöÄ –ó–ê–ü–£–°–ö –°–ö–†–ò–ü–¢–ê –û–ë–£–ß–ï–ù–ò–Ø –° GPU")
    print("=" * 50)
    
    trainer = SIXRayTrainer(
        data_yaml_path=data_yaml_path,
        model_type=args.model
    )
    
    try:
        results, best_model = trainer.train(
            epochs=args.epochs,
            batch=args.batch,
            imgsz=args.imgsz
        )
        print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()